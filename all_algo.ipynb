{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_num_reward = 3\n",
      "state_num_penalty = 0\n",
      "p = 0.65\n",
      "gamma = 0.5\n"
     ]
    }
   ],
   "source": [
    "x=8\n",
    "y=6\n",
    "z=4\n",
    "state_num_reward = ((z+1)%3)+1\n",
    "state_num_penalty = 0\n",
    "r_reward = 10\n",
    "r_penalty = -100\n",
    "r_transition = -1\n",
    "p = 0.25+0.5*x/10.0\n",
    "gamma = 0.2+0.5*y/10.0\n",
    "\n",
    "print('state_num_reward =',state_num_reward)\n",
    "print('state_num_penalty =',state_num_penalty)\n",
    "print('p =',p)\n",
    "print('gamma =',gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, s11 is renamed to be s0, all other states numbers are the same as the assignment\n",
    "\n",
    "class Gridmap_mdp:\n",
    "    def __init__(self,grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma):\n",
    "        #initialize all of the attributes\n",
    "        self.state_num_reward = state_num_reward\n",
    "        self.state_num_penalty = state_num_penalty\n",
    "        self.r_reward = r_reward\n",
    "        self.r_penalty = r_penalty\n",
    "        self.r_transition = r_transition\n",
    "        self.p = p\n",
    "        self.gamma = gamma\n",
    "        self.grid_map = grid_map\n",
    "        \n",
    "        self.count = np.zeros([4,11])\n",
    "        self.policy = np.ones([4,11])*0.25\n",
    "        self.action_value = np.zeros([4,11])\n",
    "        self.value = np.zeros([11])\n",
    "        self.locations_dict = self.create_location_dict(self.grid_map)\n",
    "        self.transition_mat = self.init_transition_mat()\n",
    "        \n",
    "    #action: 0=up 1=down 2=left 3=right\n",
    "    def init_transition_mat(self):\n",
    "        #create the transition matrix\n",
    "        transition_mat = []\n",
    "        for action in ['u','d','l','r']:\n",
    "            transition_mat.append(self.init_transition_mat_direction(action))\n",
    "        return transition_mat\n",
    "\n",
    "    def init_transition_mat_direction(self,desired_action):\n",
    "        #create a transition matrix for only for the desired_action\n",
    "        action_list = ['u','d','l','r']\n",
    "        transition_mat = np.zeros([11,11])\n",
    "        for from_ in range(11):\n",
    "            for action in action_list:\n",
    "                update_from, update_to = self.increment_where(from_,action)\n",
    "                if action == desired_action:\n",
    "                    transition_mat[update_to,update_from] += self.p\n",
    "                else:\n",
    "                    transition_mat[update_to,update_from] += (1-self.p)/3.0\n",
    "        return transition_mat\n",
    "        \n",
    "    def create_location_dict(self,grid_map):\n",
    "        #create a dictionary to map state to coordinates\n",
    "        locations_dict = {}\n",
    "        for i in range(grid_map.shape[0]):\n",
    "            for j in range(grid_map.shape[1]):\n",
    "                # store as x and y\n",
    "                locations_dict[grid_map[i][j]] = [j,i]\n",
    "        return locations_dict\n",
    "        \n",
    "    def increment_where(self,from_,direction):\n",
    "        #find where should the probability be increamented, used to help create transition matrix\n",
    "        [x,y] = self.locations_dict[from_]\n",
    "        if direction == 'r':\n",
    "            #indended direction: right\n",
    "            if x+1 >= 4 or self.grid_map[y,x+1] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y,x+1]]\n",
    "        if direction == 'l':\n",
    "            #indended direction: left\n",
    "            if x-1 < 0 or self.grid_map[y,x-1] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y,x-1]]\n",
    "        if direction == 'u':\n",
    "            #indended direction: up\n",
    "            if y-1 < 0 or self.grid_map[y-1,x] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y-1,x]]\n",
    "        if direction == 'd':\n",
    "            #indended direction: down\n",
    "            if y+1 >= 4 or self.grid_map[y+1,x] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y+1,x]]\n",
    "    \n",
    "    def get_reward(self,from_,to_):\n",
    "        #calculate the reward gained from moving from a state to another\n",
    "        if from_ == state_num_reward:\n",
    "            return self.r_reward\n",
    "        if from_ == state_num_penalty:\n",
    "            return self.r_penalty\n",
    "        if to_ == state_num_penalty:\n",
    "            return self.r_penalty\n",
    "        elif to_ == state_num_reward:\n",
    "            return self.r_reward\n",
    "        else:\n",
    "            return self.r_transition\n",
    "        \n",
    "    def get_possible_state_from(self,state_from,action):\n",
    "        #find what states are possible from a state and action\n",
    "        possible_states = []\n",
    "        #0=up 1=down 2=left 3=right\n",
    "        transition_mat = self.transition_mat[action]\n",
    "        for i in range(transition_mat.shape[1]):\n",
    "            if transition_mat[i,state_from] > 0.0:\n",
    "                state_to = i\n",
    "                p_to_that_state = transition_mat[i,state_from]\n",
    "                possible_states.append(np.array([state_to,p_to_that_state]))\n",
    "        return possible_states\n",
    "\n",
    "    def sample_mc_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        self.alpha = 0.5\n",
    "        initial_state = np.random.choice(9, 1)[0]+2\n",
    "        state = initial_state\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        return_ = 0\n",
    "        \n",
    "        while(state != state_num_penalty and state != self.state_num_reward):\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.policy[:,state])[0]\n",
    "            \n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            \n",
    "            next_state = np.random.choice(11,1,p=self.transition_mat[action][:,state])[0]\n",
    "            reward = self.get_reward(state,next_state)\n",
    "                            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            return_ += reward\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        return states,actions,rewards\n",
    "        \n",
    "    def calculate_return_from_episode(self,rewards):\n",
    "        #calculate the return from the generated episode\n",
    "        return_ = 0\n",
    "        for i,reward in enumerate(rewards):\n",
    "            return_ += self.gamma**i * reward\n",
    "        return return_\n",
    "            \n",
    "    def update_mc_episode(self,states,actions,rewards):\n",
    "        #update the action value of the agent wrt to an episode\n",
    "        for state,action in zip(states,actions):\n",
    "            self.count[action,state] += 1\n",
    "            return_ = self.calculate_return_from_episode(rewards)\n",
    "            self.action_value[action,state] = ((self.count[action,state]-1)*self.action_value[action,state] + return_)/(self.count[action,state]*1.0)\n",
    "        \n",
    "    def mc_policy_eval(self,n,epsilon=0.0):\n",
    "        #evaluate the action value function of a policy\n",
    "        for i in range(n):\n",
    "            if i % 1000 == 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_mc_episode(epsilon)\n",
    "            self.update_mc_episode(states,actions,rewards)\n",
    "        \n",
    "    def policy_improve(self):\n",
    "        #improve the policy from the action value function\n",
    "        self.policy = np.zeros_like(self.policy)\n",
    "        for state in range(11):\n",
    "            v_up = Gridmap.action_value[0,state]\n",
    "            v_down = Gridmap.action_value[1,state]\n",
    "            v_left = Gridmap.action_value[2,state]\n",
    "            v_right = Gridmap.action_value[3,state]\n",
    "            v_max = np.argmax(np.array([v_up,v_down,v_left,v_right]),0)\n",
    "            self.policy[v_max,state] = 1\n",
    "            \n",
    "    def tell_policy(self):\n",
    "        #print the policy\n",
    "        action_word = {0:'up',1:'down',2:'left',3:'right'}\n",
    "        for state in range(11):\n",
    "            if state == self.state_num_reward or state == self.state_num_penalty:\n",
    "                continue\n",
    "            v_up = Gridmap.policy[0,state]\n",
    "            v_down = Gridmap.policy[1,state]\n",
    "            v_left = Gridmap.policy[2,state]\n",
    "            v_right = Gridmap.policy[3,state]\n",
    "            v_max = np.argmax(np.array([v_up,v_down,v_left,v_right]),0)\n",
    "            print('on state =',state,', do action =',action_word[v_max])\n",
    "            \n",
    "    def mc_learn(self,n,episode_per_step,epsilon=0.0):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.action_value = np.zeros([4,11])\n",
    "            print('===============================')\n",
    "            print('>',i,'th iter')\n",
    "            print('===============================')\n",
    "            self.mc_policy_eval(episode_per_step,epsilon)\n",
    "            self.policy_improve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_map = np.array([[1,2,3,4],\n",
    "                    [5,6,-1,7],\n",
    "                    [-1,8,9,10],\n",
    "                    [-1,-1,0,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Bellman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Bellman_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        #probability transition matrix, depends on the policy\n",
    "        self.P = np.zeros((11,11))\n",
    "        self.r = np.zeros(11)\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "        \n",
    "    def update_P_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.P[from_state,from_state] = 1\n",
    "                continue\n",
    "            for to_state in range(0,11):\n",
    "                p = 0\n",
    "                for action in range(0,4):\n",
    "                    p_action = self.gridmap.policy[action,from_state]\n",
    "                    p_move_from_action = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    p += p_action*p_move_from_action\n",
    "                    #print(\"from %s to %s doing action %s, the p_action = %s, and the p_move_from_action = %s\"%(from_state,to_state,action,p_action,p_move_from_action))\n",
    "                #print(\">>> p = %s\"%(p))\n",
    "                self.P[to_state,from_state] = p\n",
    "                       \n",
    "    def update_r_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            r = 0\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.r[from_state] = 0\n",
    "                continue\n",
    "            for action in range(0,4):\n",
    "                for to_state in range(0,11):\n",
    "                    p_policy = self.gridmap.policy[action,from_state]\n",
    "                    reward = self.gridmap.get_reward(from_state,to_state)\n",
    "                    p = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    r += p_policy*p*reward\n",
    "            self.r[from_state] = r\n",
    "            \n",
    "    def evaluate_value(self):\n",
    "        self.gridmap.value = np.matmul(np.linalg.inv(np.eye(11)-self.gridmap.gamma*self.P),self.r)\n",
    "    \n",
    "    def greedy(self):\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "        \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "    \n",
    "    def step_one_iter(self):\n",
    "        #policy eval\n",
    "        self.update_P_from_policy()\n",
    "        self.update_r_from_policy()\n",
    "        self.evaluate_value()\n",
    "        #policy improve\n",
    "        self.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bellman = Bellman_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "bellman.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bellman.step_one_iter()\n",
    "#upgrade the policy once\n",
    "bellman.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-45.58849622,  -1.31531223,   5.07968155,   7.12522205,\n",
       "         5.88219852,  -1.40976582,  -2.88971302,  -2.93556299,\n",
       "        -5.95453799, -70.13614802,  -5.95756582])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bellman.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = left\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "bellman.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dp_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        #probability transition matrix, depends on the policy\n",
    "        self.P = np.zeros((11,11))\n",
    "        self.r = np.zeros(11)\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "        \n",
    "    def update_P_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.P[from_state,from_state] = 1\n",
    "                continue\n",
    "            for to_state in range(0,11):\n",
    "                p = 0\n",
    "                for action in range(0,4):\n",
    "                    p_action = self.gridmap.policy[action,from_state]\n",
    "                    p_move_from_action = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    p += p_action*p_move_from_action\n",
    "                    #print(\"from %s to %s doing action %s, the p_action = %s, and the p_move_from_action = %s\"%(from_state,to_state,action,p_action,p_move_from_action))\n",
    "                #print(\">>> p = %s\"%(p))\n",
    "                self.P[to_state,from_state] = p\n",
    "                       \n",
    "    def update_r_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            r = 0\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.r[from_state] = 0\n",
    "                continue\n",
    "            for action in range(0,4):\n",
    "                for to_state in range(0,11):\n",
    "                    p_policy = self.gridmap.policy[action,from_state]\n",
    "                    reward = self.gridmap.get_reward(from_state,to_state)\n",
    "                    p = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    r += p_policy*p*reward\n",
    "            self.r[from_state] = r\n",
    "            \n",
    "    def evaluate_value(self):\n",
    "        error = 9999\n",
    "        i=0\n",
    "        while(error > 10**-3):\n",
    "            i+=1\n",
    "            for from_state in range(0,11):\n",
    "                v = 0\n",
    "                old_val = self.gridmap.value\n",
    "                if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                    if from_state == self.gridmap.state_num_reward:\n",
    "                        self.gridmap.value[from_state] = self.gridmap.r_reward\n",
    "                    elif from_state == self.gridmap.state_num_penalty:\n",
    "                        self.gridmap.value[from_state] = self.gridmap.r_penalty\n",
    "                    continue\n",
    "                for to_state in range(0,11):\n",
    "                    for action in range(0,4):\n",
    "                        p_action = self.gridmap.policy[action,from_state]\n",
    "                        p_move_from_action = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                        r_that_action = self.gridmap.get_reward(from_state,to_state)\n",
    "                        v += p_action*p_move_from_action*(r_that_action + self.gridmap.gamma*self.gridmap.value[to_state])\n",
    "                        #print(\"from %s to %s doing action %s, the p_action = %s, and the p_move_from_action = %s\"%(from_state,to_state,action,p_action,p_move_from_action))\n",
    "                    #print(\">>> p = %s\"%(p))\n",
    "                self.gridmap.value[from_state] = v\n",
    "            error = np.max(np.absolute(np.array(old_val)-np.array(self.gridmap.value)))\n",
    "            if(i%1 == 0):\n",
    "                print(\"i = %s, error = %s\"%(i,error))\n",
    "    \n",
    "    def greedy(self):\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "        \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "    \n",
    "    def step_one_iter(self):\n",
    "        #policy eval\n",
    "        self.update_P_from_policy()\n",
    "        self.update_r_from_policy()\n",
    "        self.evaluate_value()\n",
    "        #policy improve\n",
    "        self.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = Dp_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "dp.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1, error = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.step_one_iter()\n",
    "#upgrade the policy once\n",
    "dp.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.00000000e+02,  2.64724305e+00,  1.02935521e+01,  1.00000000e+01,\n",
       "        1.08219326e+01, -4.63929217e-03,  2.39086665e+00,  2.75217212e+00,\n",
       "       -1.58435120e+00, -2.01244131e+01, -1.44952608e+00])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = right\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "dp.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Mc_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        self.alpha = 0.5\n",
    "        self.count = np.zeros(11)\n",
    "        self.k = 0\n",
    "        self.gridmap.value[self.gridmap.state_num_reward] = self.gridmap.r_reward\n",
    "        self.gridmap.value[self.gridmap.state_num_penalty] = self.gridmap.r_penalty\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "\n",
    "    def sample_mc_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        initial_state = random.randint(0,10)\n",
    "        while initial_state == self.gridmap.state_num_reward or initial_state == self.gridmap.state_num_penalty:\n",
    "            initial_state = random.randint(0,10)\n",
    "            \n",
    "        state = initial_state\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        return_ = 0\n",
    "        while(state != self.gridmap.state_num_penalty and state != self.gridmap.state_num_reward):\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.gridmap.policy[:,state])[0]\n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            next_state = np.random.choice(11,1,p=self.gridmap.transition_mat[action][:,state])[0]\n",
    "            reward = self.gridmap.get_reward(state,next_state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        return states,actions,rewards\n",
    "        \n",
    "    def calculate_return_from_episode(self,rewards):\n",
    "        #calculate the return from the generated episode\n",
    "        return_ = 0\n",
    "        for i,reward in enumerate(rewards):\n",
    "            return_ += self.gridmap.gamma**i * reward\n",
    "        return return_\n",
    "            \n",
    "    def update_mc_episode(self,states,actions,rewards):\n",
    "        #update the value of the agent wrt to an episode\n",
    "        #update only for the first visit\n",
    "        visited = []\n",
    "        for state in states:\n",
    "            if not state in visited:\n",
    "                visited.append(state)                \n",
    "                self.count[state] += 1\n",
    "                return_ = self.calculate_return_from_episode(rewards)\n",
    "                self.gridmap.value[state] += (return_-self.gridmap.value[state])/(self.count[state]*1.0)\n",
    "        \n",
    "    def mc_prediction(self,n,epsilon=0.0):\n",
    "        #evaluate the value function of a policy by finding a sample and updating\n",
    "        for i in range(n):\n",
    "            if i % 1000 == 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_mc_episode(epsilon)\n",
    "            self.update_mc_episode(states,actions,rewards)\n",
    "        \n",
    "    def mc_control(self):\n",
    "        #improve the policy from the value function\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "            \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "        \n",
    "           \n",
    "    def mc_iterate(self,n,episode_per_step):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.k += 1\n",
    "            self.value = np.zeros(11)\n",
    "            print('===============================')\n",
    "            print('>',i,'th iter')\n",
    "            print('===============================')\n",
    "            self.mc_prediction(episode_per_step,epsilon=1.0/self.k)\n",
    "            self.mc_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = Mc_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "mc.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 0 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 1 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 2 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 3 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 4 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 5 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 6 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 7 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 8 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 9 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 10 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 11 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 12 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 13 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 14 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 15 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 16 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 17 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 18 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 19 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 20 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 21 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 22 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 23 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 24 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 25 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 26 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 27 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 28 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 29 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 30 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 31 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 32 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 33 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 34 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 35 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 36 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 37 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 38 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 39 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 40 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 41 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 42 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 43 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 44 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 45 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 46 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 47 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 48 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 49 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 50 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 51 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 52 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 53 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 54 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 55 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 56 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 57 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 58 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 59 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 60 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 61 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 62 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 63 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 64 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 65 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 66 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 67 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 68 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 69 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 70 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 71 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 72 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 73 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 74 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 75 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 76 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 77 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 78 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 79 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 80 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 81 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 82 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 83 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 84 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 85 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 86 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 87 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 88 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 89 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 90 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 91 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 92 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 93 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 94 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 95 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 96 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 97 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 98 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 99 th iter\n",
      "===============================\n",
      "performing 0th episode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc.mc_iterate(100,1)\n",
    "#upgrade the policy once\n",
    "mc.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100.        ,   -0.39270237,    1.174625  ,   10.        ,\n",
       "          0.62446092,   -1.02195485,   -0.50838228,   -0.69277148,\n",
       "         -1.27475647,  -15.82984234,   -1.03393309])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = down\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = right\n",
      "on state = 6 , do action = left\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = right\n",
      "on state = 9 , do action = right\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "mc.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Td_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        self.alpha = 0.5\n",
    "        self.count = np.zeros(11)\n",
    "        self.k = 0\n",
    "        self.gridmap.value[self.gridmap.state_num_reward] = self.gridmap.r_reward\n",
    "        self.gridmap.value[self.gridmap.state_num_penalty] = self.gridmap.r_penalty\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "\n",
    "    def sample_td_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        initial_state = random.randint(0,10)\n",
    "        while initial_state == self.gridmap.state_num_reward or initial_state == self.gridmap.state_num_penalty:\n",
    "            initial_state = random.randint(0,10)\n",
    "            \n",
    "        state = initial_state\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        return_ = 0\n",
    "        while(state != self.gridmap.state_num_penalty and state != self.gridmap.state_num_reward):\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.gridmap.policy[:,state])[0]\n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            next_state = np.random.choice(11,1,p=self.gridmap.transition_mat[action][:,state])[0]\n",
    "            reward = self.gridmap.get_reward(state,next_state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        return states,actions,rewards\n",
    "        \n",
    "    def calculate_return_from_episode(self,rewards):\n",
    "        #calculate the return from the generated episode\n",
    "        return_ = 0\n",
    "        for i,reward in enumerate(rewards):\n",
    "            return_ += self.gridmap.gamma**i * reward\n",
    "        return return_\n",
    "            \n",
    "    def update_td_episode(self,states,actions,rewards):\n",
    "        #update the value of the agent wrt to an episode\n",
    "        #update only for the first visit\n",
    "        visited = []\n",
    "        for (i,state) in enumerate(states):\n",
    "            if i+1 == len(states):\n",
    "                break\n",
    "            if not state in visited:\n",
    "                visited.append(state)                \n",
    "                self.count[state] += 1\n",
    "                bootstrapped_return = rewards[i]+self.gridmap.gamma*self.gridmap.value[states[i+1]]\n",
    "                self.gridmap.value[state] += (bootstrapped_return-self.gridmap.value[state])/(self.count[state]*1.0)\n",
    "        \n",
    "    def td_prediction(self,n,epsilon=0.0):\n",
    "        #evaluate the value function of a policy by finding a sample and updating\n",
    "        for i in range(n):\n",
    "            if i % 1000 == 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_td_episode(epsilon)\n",
    "            self.update_td_episode(states,actions,rewards)\n",
    "        \n",
    "    def td_control(self):\n",
    "        #improve the policy from the value function\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "            \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "        \n",
    "           \n",
    "    def td_iterate(self,n,episode_per_step):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.k += 1\n",
    "            self.value = np.zeros(11)\n",
    "            print('===============================')\n",
    "            print('>',i,'th iter')\n",
    "            print('===============================')\n",
    "            self.td_prediction(episode_per_step,epsilon=1.0/self.k)\n",
    "            self.td_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = Td_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "td.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 0 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 1 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 2 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 3 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 4 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 5 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 6 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 7 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 8 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 9 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 10 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 11 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 12 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 13 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 14 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 15 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 16 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 17 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 18 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 19 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 20 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 21 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 22 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 23 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 24 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 25 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 26 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 27 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 28 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 29 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 30 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 31 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 32 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 33 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 34 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 35 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 36 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 37 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 38 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 39 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 40 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 41 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 42 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 43 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 44 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 45 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 46 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 47 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 48 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 49 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 50 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 51 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 52 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 53 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 54 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 55 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 56 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 57 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 58 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 59 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 60 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 61 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 62 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 63 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 64 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 65 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 66 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 67 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 68 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 69 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 70 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 71 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 72 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 73 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 74 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 75 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 76 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 77 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 78 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 79 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 80 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 81 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 82 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 83 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 84 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 85 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 86 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 87 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 88 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 89 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 90 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 91 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 92 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 93 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 94 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 95 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 96 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 97 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 98 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 99 th iter\n",
      "===============================\n",
      "performing 0th episode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.td_iterate(100,1)\n",
    "#upgrade the policy once\n",
    "td.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100.        ,   -1.75269257,   -1.73343219,   10.        ,\n",
       "         -1.68731074,   -1.76322022,   -1.76319135,   -1.65991254,\n",
       "         -1.75816304,   -1.76339028,   -1.73968005])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = right\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "td.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Sarsa_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        self.alpha = 0.5\n",
    "        self.count = np.zeros((4,11))\n",
    "        self.k = 0\n",
    "        self.gridmap.value[self.gridmap.state_num_reward] = self.gridmap.r_reward\n",
    "        self.gridmap.value[self.gridmap.state_num_penalty] = self.gridmap.r_penalty\n",
    "\n",
    "    def sample_sarsa_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        initial_state = random.randint(0,10)\n",
    "        while initial_state == self.gridmap.state_num_reward or initial_state == self.gridmap.state_num_penalty:\n",
    "            initial_state = random.randint(0,10)\n",
    "            \n",
    "        state = initial_state\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        return_ = 0\n",
    "        while(state != self.gridmap.state_num_penalty and state != self.gridmap.state_num_reward):\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.gridmap.policy[:,state])[0]\n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            next_state = np.random.choice(11,1,p=self.gridmap.transition_mat[action][:,state])[0]\n",
    "            reward = self.gridmap.get_reward(state,next_state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        return states,actions,rewards\n",
    "        \n",
    "    def calculate_return_from_episode(self,rewards):\n",
    "        #calculate the return from the generated episode\n",
    "        return_ = 0\n",
    "        for i,reward in enumerate(rewards):\n",
    "            return_ += self.gridmap.gamma**i * reward\n",
    "        return return_\n",
    "            \n",
    "    def update_sarsa_episode(self,states,actions,rewards):\n",
    "        #update the action value of the agent wrt to an episode\n",
    "        #update only for the first visit\n",
    "        visited = []\n",
    "        for (i,(state,action)) in enumerate(zip(states,actions)):\n",
    "            state_action_code =str(state)+str(action)\n",
    "            if i+1 == len(states):\n",
    "                break\n",
    "            #if not state_action_code in visited:\n",
    "            alpha = 1.0/self.k**2\n",
    "            visited.append(state_action_code)                \n",
    "            self.count[action,state] += 1\n",
    "            next_state = int(states[i+1])\n",
    "            next_action = int(actions[i+1])\n",
    "            next_q = float(self.gridmap.action_value[next_action,next_state])\n",
    "            bootstrapped_return = rewards[i]+self.gridmap.gamma*next_q\n",
    "            #print(\"alpha = %s: from %s, do action %s (action value = %s)\"%(alpha,state,action,self.gridmap.action_value[action,state]))\n",
    "            #print(next_state)\n",
    "            #print(\"do action %s, reward = %s (next action value = %s)\"%(next_action,rewards[i],next_q))\n",
    "            self.gridmap.action_value[action,state] += alpha*(bootstrapped_return-self.gridmap.action_value[action,state])\n",
    "\n",
    "    def sarsa_prediction(self,n,epsilon=0.0):\n",
    "        #evaluate the action value function of a policy by finding a sample and updating\n",
    "        for i in range(n):\n",
    "            if i % 5000 == 0 and i != 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_sarsa_episode(epsilon)\n",
    "            self.update_sarsa_episode(states,actions,rewards)\n",
    "            \n",
    "    def sarsa_control(self):\n",
    "        #improve the policy from the action value function\n",
    "        self.gridmap.policy = np.zeros_like(self.gridmap.policy)\n",
    "        for state in range(11):\n",
    "            q_up = self.gridmap.action_value[0,state]\n",
    "            q_down = self.gridmap.action_value[1,state]\n",
    "            q_left = self.gridmap.action_value[2,state]\n",
    "            q_right = self.gridmap.action_value[3,state]\n",
    "            q_max = np.argmax(np.array([q_up,q_down,q_left,q_right]),0)\n",
    "            self.gridmap.policy[q_max,state] = 1\n",
    "           \n",
    "    def sarsa_iterate(self,n,episode_per_step):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.k += 1\n",
    "            self.value = np.zeros(11)\n",
    "            if i%1000 == 0:\n",
    "                print('===============================')\n",
    "                print('>',i,'th iter')\n",
    "                print('===============================')\n",
    "            self.sarsa_prediction(episode_per_step,epsilon=1.0/self.k)\n",
    "            self.sarsa_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = Sarsa_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa.gridmap.action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "sarsa.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 0 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 1000 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 2000 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 3000 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 4000 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 5000 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 6000 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 7000 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 8000 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 9000 th iter\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.]])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa.sarsa_iterate(10000,1)\n",
    "#upgrade the policy once\n",
    "sarsa.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.   -1.   -0.06  0.   -0.47 -0.15 -0.1  -0.58 -0.14 -0.32 -0.31]\n",
      " [ 0.   -0.21 -0.04  0.   -0.11 -0.13 -0.1  -0.45 -0.04 -0.05 -0.13]\n",
      " [ 0.   -0.24 -0.04  0.   -0.25 -0.5  -0.13 -0.4  -0.2  -0.06 -0.13]\n",
      " [ 0.   -1.   -0.04  0.   -0.11 -0.09 -0.1  -0.4  -0.04 -0.07 -0.14]]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(sarsa.gridmap.action_value,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = down\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = right\n",
      "on state = 5 , do action = right\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = right\n",
      "on state = 8 , do action = right\n",
      "on state = 9 , do action = down\n",
      "on state = 10 , do action = left\n"
     ]
    }
   ],
   "source": [
    "sarsa.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
