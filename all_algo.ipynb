{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_num_reward = 3\n",
      "state_num_penalty = 0\n",
      "p = 0.65\n",
      "gamma = 0.5\n"
     ]
    }
   ],
   "source": [
    "x=8\n",
    "y=6\n",
    "z=4\n",
    "state_num_reward = ((z+1)%3)+1\n",
    "state_num_penalty = 0\n",
    "r_reward = 10\n",
    "r_penalty = -100\n",
    "r_transition = -1\n",
    "p = 0.25+0.5*x/10.0\n",
    "gamma = 0.2+0.5*y/10.0\n",
    "\n",
    "print('state_num_reward =',state_num_reward)\n",
    "print('state_num_penalty =',state_num_penalty)\n",
    "print('p =',p)\n",
    "print('gamma =',gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, s11 is renamed to be s0, all other states numbers are the same as the assignment\n",
    "\n",
    "class Gridmap_mdp:\n",
    "    def __init__(self,grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma):\n",
    "        #initialize all of the attributes\n",
    "        self.state_num_reward = state_num_reward\n",
    "        self.state_num_penalty = state_num_penalty\n",
    "        self.r_reward = r_reward\n",
    "        self.r_penalty = r_penalty\n",
    "        self.r_transition = r_transition\n",
    "        self.p = p\n",
    "        self.gamma = gamma\n",
    "        self.grid_map = grid_map\n",
    "        \n",
    "        self.count = np.zeros([4,11])\n",
    "        self.policy = np.ones([4,11])*0.25\n",
    "        self.action_value = np.zeros([4,11])\n",
    "        self.value = np.zeros([11])\n",
    "        self.locations_dict = self.create_location_dict(self.grid_map)\n",
    "        self.transition_mat = self.init_transition_mat()\n",
    "        \n",
    "    #action: 0=up 1=down 2=left 3=right\n",
    "    def init_transition_mat(self):\n",
    "        #create the transition matrix\n",
    "        transition_mat = []\n",
    "        for action in ['u','d','l','r']:\n",
    "            transition_mat.append(self.init_transition_mat_direction(action))\n",
    "        return transition_mat\n",
    "\n",
    "    def init_transition_mat_direction(self,desired_action):\n",
    "        #create a transition matrix for only for the desired_action\n",
    "        action_list = ['u','d','l','r']\n",
    "        transition_mat = np.zeros([11,11])\n",
    "        for from_ in range(11):\n",
    "            for action in action_list:\n",
    "                update_from, update_to = self.increment_where(from_,action)\n",
    "                if action == desired_action:\n",
    "                    transition_mat[update_to,update_from] += self.p\n",
    "                else:\n",
    "                    transition_mat[update_to,update_from] += (1-self.p)/3.0\n",
    "        return transition_mat\n",
    "        \n",
    "    def create_location_dict(self,grid_map):\n",
    "        #create a dictionary to map state to coordinates\n",
    "        locations_dict = {}\n",
    "        for i in range(grid_map.shape[0]):\n",
    "            for j in range(grid_map.shape[1]):\n",
    "                # store as x and y\n",
    "                locations_dict[grid_map[i][j]] = [j,i]\n",
    "        return locations_dict\n",
    "        \n",
    "    def increment_where(self,from_,direction):\n",
    "        #find where should the probability be increamented, used to help create transition matrix\n",
    "        [x,y] = self.locations_dict[from_]\n",
    "        if direction == 'r':\n",
    "            #indended direction: right\n",
    "            if x+1 >= 4 or self.grid_map[y,x+1] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y,x+1]]\n",
    "        if direction == 'l':\n",
    "            #indended direction: left\n",
    "            if x-1 < 0 or self.grid_map[y,x-1] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y,x-1]]\n",
    "        if direction == 'u':\n",
    "            #indended direction: up\n",
    "            if y-1 < 0 or self.grid_map[y-1,x] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y-1,x]]\n",
    "        if direction == 'd':\n",
    "            #indended direction: down\n",
    "            if y+1 >= 4 or self.grid_map[y+1,x] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y+1,x]]\n",
    "    \n",
    "    def get_reward(self,from_,to_):\n",
    "        #calculate the reward gained from moving from a state to another\n",
    "        if from_ == state_num_reward:\n",
    "            return self.r_reward\n",
    "        if from_ == state_num_penalty:\n",
    "            return self.r_penalty\n",
    "        if to_ == state_num_penalty:\n",
    "            return self.r_penalty\n",
    "        elif to_ == state_num_reward:\n",
    "            return self.r_reward\n",
    "        else:\n",
    "            return self.r_transition\n",
    "        \n",
    "    def get_possible_state_from(self,state_from,action):\n",
    "        #find what states are possible from a state and action\n",
    "        possible_states = []\n",
    "        #0=up 1=down 2=left 3=right\n",
    "        transition_mat = self.transition_mat[action]\n",
    "        for i in range(transition_mat.shape[1]):\n",
    "            if transition_mat[i,state_from] > 0.0:\n",
    "                state_to = i\n",
    "                p_to_that_state = transition_mat[i,state_from]\n",
    "                possible_states.append(np.array([state_to,p_to_that_state]))\n",
    "        return possible_states\n",
    "\n",
    "    def sample_mc_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        self.alpha = 0.5\n",
    "        initial_state = np.random.choice(9, 1)[0]+2\n",
    "        state = initial_state\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        return_ = 0\n",
    "        \n",
    "        while(state != state_num_penalty and state != self.state_num_reward):\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.policy[:,state])[0]\n",
    "            \n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            \n",
    "            next_state = np.random.choice(11,1,p=self.transition_mat[action][:,state])[0]\n",
    "            reward = self.get_reward(state,next_state)\n",
    "                            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            return_ += reward\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        return states,actions,rewards\n",
    "        \n",
    "    def calculate_return_from_episode(self,rewards):\n",
    "        #calculate the return from the generated episode\n",
    "        return_ = 0\n",
    "        for i,reward in enumerate(rewards):\n",
    "            return_ += self.gamma**i * reward\n",
    "        return return_\n",
    "            \n",
    "    def update_mc_episode(self,states,actions,rewards):\n",
    "        #update the action value of the agent wrt to an episode\n",
    "        for state,action in zip(states,actions):\n",
    "            self.count[action,state] += 1\n",
    "            return_ = self.calculate_return_from_episode(rewards)\n",
    "            self.action_value[action,state] = ((self.count[action,state]-1)*self.action_value[action,state] + return_)/(self.count[action,state]*1.0)\n",
    "        \n",
    "    def mc_policy_eval(self,n,epsilon=0.0):\n",
    "        #evaluate the action value function of a policy\n",
    "        for i in range(n):\n",
    "            if i % 1000 == 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_mc_episode(epsilon)\n",
    "            self.update_mc_episode(states,actions,rewards)\n",
    "        \n",
    "    def policy_improve(self):\n",
    "        #improve the policy from the action value function\n",
    "        self.policy = np.zeros_like(self.policy)\n",
    "        for state in range(11):\n",
    "            v_up = Gridmap.action_value[0,state]\n",
    "            v_down = Gridmap.action_value[1,state]\n",
    "            v_left = Gridmap.action_value[2,state]\n",
    "            v_right = Gridmap.action_value[3,state]\n",
    "            v_max = np.argmax(np.array([v_up,v_down,v_left,v_right]),0)\n",
    "            self.policy[v_max,state] = 1\n",
    "            \n",
    "    def tell_policy(self):\n",
    "        #print the policy\n",
    "        action_word = {0:'up',1:'down',2:'left',3:'right'}\n",
    "        for state in range(11):\n",
    "            if state == self.state_num_reward or state == self.state_num_penalty:\n",
    "                continue\n",
    "            v_up = Gridmap.policy[0,state]\n",
    "            v_down = Gridmap.policy[1,state]\n",
    "            v_left = Gridmap.policy[2,state]\n",
    "            v_right = Gridmap.policy[3,state]\n",
    "            v_max = np.argmax(np.array([v_up,v_down,v_left,v_right]),0)\n",
    "            print('on state =',state,', do action =',action_word[v_max])\n",
    "            \n",
    "    def mc_learn(self,n,episode_per_step,epsilon=0.0):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.action_value = np.zeros([4,11])\n",
    "            print('===============================')\n",
    "            print('>',i,'th iter')\n",
    "            print('===============================')\n",
    "            self.mc_policy_eval(episode_per_step,epsilon)\n",
    "            self.policy_improve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_map = np.array([[1,2,3,4],\n",
    "                    [5,6,-1,7],\n",
    "                    [-1,8,9,10],\n",
    "                    [-1,-1,0,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Bellman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Bellman_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        #probability transition matrix, depends on the policy\n",
    "        self.P = np.zeros((11,11))\n",
    "        self.r = np.zeros(11)\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "        \n",
    "    def update_P_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.P[from_state,from_state] = 1\n",
    "                continue\n",
    "            for to_state in range(0,11):\n",
    "                p = 0\n",
    "                for action in range(0,4):\n",
    "                    p_action = self.gridmap.policy[action,from_state]\n",
    "                    p_move_from_action = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    p += p_action*p_move_from_action\n",
    "                    #print(\"from %s to %s doing action %s, the p_action = %s, and the p_move_from_action = %s\"%(from_state,to_state,action,p_action,p_move_from_action))\n",
    "                #print(\">>> p = %s\"%(p))\n",
    "                self.P[to_state,from_state] = p\n",
    "                       \n",
    "    def update_r_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            r = 0\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.r[from_state] = 0\n",
    "                continue\n",
    "            for action in range(0,4):\n",
    "                for to_state in range(0,11):\n",
    "                    p_policy = self.gridmap.policy[action,from_state]\n",
    "                    reward = self.gridmap.get_reward(from_state,to_state)\n",
    "                    p = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    r += p_policy*p*reward\n",
    "            self.r[from_state] = r\n",
    "            \n",
    "    def evaluate_value(self):\n",
    "        self.gridmap.value = np.matmul(np.linalg.inv(np.eye(11)-self.gridmap.gamma*self.P),self.r)\n",
    "    \n",
    "    def greedy(self):\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "        \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "    \n",
    "    def step_one_iter(self):\n",
    "        #policy eval\n",
    "        self.update_P_from_policy()\n",
    "        self.update_r_from_policy()\n",
    "        self.evaluate_value()\n",
    "        #policy improve\n",
    "        self.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "bellman = Bellman_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "bellman.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bellman.step_one_iter()\n",
    "#upgrade the policy once\n",
    "bellman.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-45.58849622,  -1.31531223,   5.07968155,   7.12522205,\n",
       "         5.88219852,  -1.40976582,  -2.88971302,  -2.93556299,\n",
       "        -5.95453799, -70.13614802,  -5.95756582])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bellman.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = left\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "bellman.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
