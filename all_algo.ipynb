{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_num_reward = 3\n",
      "state_num_penalty = 0\n",
      "p = 0.65\n",
      "gamma = 0.5\n"
     ]
    }
   ],
   "source": [
    "x=8\n",
    "y=6\n",
    "z=4\n",
    "state_num_reward = ((z+1)%3)+1\n",
    "state_num_penalty = 0\n",
    "r_reward = 10\n",
    "r_penalty = -100\n",
    "r_transition = -1\n",
    "p = 0.25+0.5*x/10.0\n",
    "gamma = 0.2+0.5*y/10.0\n",
    "\n",
    "print('state_num_reward =',state_num_reward)\n",
    "print('state_num_penalty =',state_num_penalty)\n",
    "print('p =',p)\n",
    "print('gamma =',gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, s11 is renamed to be s0, all other states numbers are the same as the assignment\n",
    "\n",
    "class Gridmap_mdp:\n",
    "    def __init__(self,grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma):\n",
    "        #initialize all of the attributes\n",
    "        self.state_num_reward = state_num_reward\n",
    "        self.state_num_penalty = state_num_penalty\n",
    "        self.r_reward = r_reward\n",
    "        self.r_penalty = r_penalty\n",
    "        self.r_transition = r_transition\n",
    "        self.p = p\n",
    "        self.gamma = gamma\n",
    "        self.grid_map = grid_map\n",
    "        \n",
    "        self.count = np.zeros([4,11])\n",
    "        self.policy = np.ones([4,11])*0.25\n",
    "        self.action_value = np.zeros([4,11])\n",
    "        self.value = np.zeros([11])\n",
    "        self.locations_dict = self.create_location_dict(self.grid_map)\n",
    "        self.transition_mat = self.init_transition_mat()\n",
    "        \n",
    "    #action: 0=up 1=down 2=left 3=right\n",
    "    def init_transition_mat(self):\n",
    "        #create the transition matrix\n",
    "        transition_mat = []\n",
    "        for action in ['u','d','l','r']:\n",
    "            transition_mat.append(self.init_transition_mat_direction(action))\n",
    "        return transition_mat\n",
    "\n",
    "    def init_transition_mat_direction(self,desired_action):\n",
    "        #create a transition matrix for only for the desired_action\n",
    "        action_list = ['u','d','l','r']\n",
    "        transition_mat = np.zeros([11,11])\n",
    "        for from_ in range(11):\n",
    "            for action in action_list:\n",
    "                update_from, update_to = self.increment_where(from_,action)\n",
    "                if action == desired_action:\n",
    "                    transition_mat[update_to,update_from] += self.p\n",
    "                else:\n",
    "                    transition_mat[update_to,update_from] += (1-self.p)/3.0\n",
    "        return transition_mat\n",
    "        \n",
    "    def create_location_dict(self,grid_map):\n",
    "        #create a dictionary to map state to coordinates\n",
    "        locations_dict = {}\n",
    "        for i in range(grid_map.shape[0]):\n",
    "            for j in range(grid_map.shape[1]):\n",
    "                # store as x and y\n",
    "                locations_dict[grid_map[i][j]] = [j,i]\n",
    "        return locations_dict\n",
    "        \n",
    "    def increment_where(self,from_,direction):\n",
    "        #find where should the probability be increamented, used to help create transition matrix\n",
    "        [x,y] = self.locations_dict[from_]\n",
    "        if direction == 'r':\n",
    "            #indended direction: right\n",
    "            if x+1 >= 4 or self.grid_map[y,x+1] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y,x+1]]\n",
    "        if direction == 'l':\n",
    "            #indended direction: left\n",
    "            if x-1 < 0 or self.grid_map[y,x-1] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y,x-1]]\n",
    "        if direction == 'u':\n",
    "            #indended direction: up\n",
    "            if y-1 < 0 or self.grid_map[y-1,x] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y-1,x]]\n",
    "        if direction == 'd':\n",
    "            #indended direction: down\n",
    "            if y+1 >= 4 or self.grid_map[y+1,x] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y+1,x]]\n",
    "    \n",
    "    def get_reward(self,from_,to_):\n",
    "        #calculate the reward gained from moving from a state to another\n",
    "        if from_ == state_num_reward:\n",
    "            return self.r_reward\n",
    "        if from_ == state_num_penalty:\n",
    "            return self.r_penalty\n",
    "        if to_ == state_num_penalty:\n",
    "            return self.r_penalty\n",
    "        elif to_ == state_num_reward:\n",
    "            return self.r_reward\n",
    "        else:\n",
    "            return self.r_transition\n",
    "        \n",
    "    def get_possible_state_from(self,state_from,action):\n",
    "        #find what states are possible from a state and action\n",
    "        possible_states = []\n",
    "        #0=up 1=down 2=left 3=right\n",
    "        transition_mat = self.transition_mat[action]\n",
    "        for i in range(transition_mat.shape[1]):\n",
    "            if transition_mat[i,state_from] > 0.0:\n",
    "                state_to = i\n",
    "                p_to_that_state = transition_mat[i,state_from]\n",
    "                possible_states.append(np.array([state_to,p_to_that_state]))\n",
    "        return possible_states\n",
    "            \n",
    "    def tell_policy(self):\n",
    "        #print the policy\n",
    "        action_word = {0:'up',1:'down',2:'left',3:'right'}\n",
    "        for state in range(11):\n",
    "            if state == self.state_num_reward or state == self.state_num_penalty:\n",
    "                continue\n",
    "            v_up = Gridmap.policy[0,state]\n",
    "            v_down = Gridmap.policy[1,state]\n",
    "            v_left = Gridmap.policy[2,state]\n",
    "            v_right = Gridmap.policy[3,state]\n",
    "            v_max = np.argmax(np.array([v_up,v_down,v_left,v_right]),0)\n",
    "            print('on state =',state,', do action =',action_word[v_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_map = np.array([[1,2,3,4],\n",
    "                    [5,6,-1,7],\n",
    "                    [-1,8,9,10],\n",
    "                    [-1,-1,0,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Bellman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Bellman_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        #probability transition matrix, depends on the policy\n",
    "        self.P = np.zeros((11,11))\n",
    "        self.r = np.zeros(11)\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "        \n",
    "    def update_P_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.P[from_state,from_state] = 1\n",
    "                continue\n",
    "            for to_state in range(0,11):\n",
    "                p = 0\n",
    "                for action in range(0,4):\n",
    "                    p_action = self.gridmap.policy[action,from_state]\n",
    "                    p_move_from_action = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    p += p_action*p_move_from_action\n",
    "                    #print(\"from %s to %s doing action %s, the p_action = %s, and the p_move_from_action = %s\"%(from_state,to_state,action,p_action,p_move_from_action))\n",
    "                #print(\">>> p = %s\"%(p))\n",
    "                self.P[to_state,from_state] = p\n",
    "                       \n",
    "    def update_r_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            r = 0\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.r[from_state] = 0\n",
    "                continue\n",
    "            for action in range(0,4):\n",
    "                for to_state in range(0,11):\n",
    "                    p_policy = self.gridmap.policy[action,from_state]\n",
    "                    reward = self.gridmap.get_reward(from_state,to_state)\n",
    "                    p = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    r += p_policy*p*reward\n",
    "            self.r[from_state] = r\n",
    "            \n",
    "    def evaluate_value(self):\n",
    "        self.gridmap.value = np.matmul(np.linalg.inv(np.eye(11)-self.gridmap.gamma*self.P),self.r)\n",
    "    \n",
    "    def greedy(self):\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "        \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "    \n",
    "    def step_one_iter(self):\n",
    "        #policy eval\n",
    "        self.update_P_from_policy()\n",
    "        self.update_r_from_policy()\n",
    "        self.evaluate_value()\n",
    "        #policy improve\n",
    "        self.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bellman = Bellman_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "bellman.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bellman.step_one_iter()\n",
    "#upgrade the policy once\n",
    "bellman.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -7.85222991,  -1.40824306,   1.48486538,   0.86458777,\n",
       "         1.97348569,  -1.93432372,  -2.19769926,  -2.15908587,\n",
       "        -6.93443648, -31.40891963,  -6.92800092])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bellman.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = right\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "bellman.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dp_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        #probability transition matrix, depends on the policy\n",
    "        self.P = np.zeros((11,11))\n",
    "        self.r = np.zeros(11)\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "        \n",
    "    def update_P_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.P[from_state,from_state] = 1\n",
    "                continue\n",
    "            for to_state in range(0,11):\n",
    "                p = 0\n",
    "                for action in range(0,4):\n",
    "                    p_action = self.gridmap.policy[action,from_state]\n",
    "                    p_move_from_action = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    p += p_action*p_move_from_action\n",
    "                    #print(\"from %s to %s doing action %s, the p_action = %s, and the p_move_from_action = %s\"%(from_state,to_state,action,p_action,p_move_from_action))\n",
    "                #print(\">>> p = %s\"%(p))\n",
    "                self.P[to_state,from_state] = p\n",
    "                       \n",
    "    def update_r_from_policy(self):\n",
    "        for from_state in range(0,11):\n",
    "            r = 0\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                self.r[from_state] = 0\n",
    "                continue\n",
    "            for action in range(0,4):\n",
    "                for to_state in range(0,11):\n",
    "                    p_policy = self.gridmap.policy[action,from_state]\n",
    "                    reward = self.gridmap.get_reward(from_state,to_state)\n",
    "                    p = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                    r += p_policy*p*reward\n",
    "            self.r[from_state] = r\n",
    "            \n",
    "    def evaluate_value(self):\n",
    "        error = 9999\n",
    "        i=0\n",
    "        while(error > 10**-3):\n",
    "            i+=1\n",
    "            for from_state in range(0,11):\n",
    "                v = 0\n",
    "                old_val = self.gridmap.value\n",
    "                if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                    if from_state == self.gridmap.state_num_reward:\n",
    "                        self.gridmap.value[from_state] = self.gridmap.r_reward\n",
    "                    elif from_state == self.gridmap.state_num_penalty:\n",
    "                        self.gridmap.value[from_state] = self.gridmap.r_penalty\n",
    "                    continue\n",
    "                for to_state in range(0,11):\n",
    "                    for action in range(0,4):\n",
    "                        p_action = self.gridmap.policy[action,from_state]\n",
    "                        p_move_from_action = self.gridmap.transition_mat[action][to_state,from_state]\n",
    "                        r_that_action = self.gridmap.get_reward(from_state,to_state)\n",
    "                        v += p_action*p_move_from_action*(r_that_action + self.gridmap.gamma*self.gridmap.value[to_state])\n",
    "                        #print(\"from %s to %s doing action %s, the p_action = %s, and the p_move_from_action = %s\"%(from_state,to_state,action,p_action,p_move_from_action))\n",
    "                    #print(\">>> p = %s\"%(p))\n",
    "                self.gridmap.value[from_state] = v\n",
    "            error = np.max(np.absolute(np.array(old_val)-np.array(self.gridmap.value)))\n",
    "            if(i%1 == 0):\n",
    "                print(\"i = %s, error = %s\"%(i,error))\n",
    "    \n",
    "    def greedy(self):\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "        \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "    \n",
    "    def step_one_iter(self):\n",
    "        #policy eval\n",
    "        self.update_P_from_policy()\n",
    "        self.update_r_from_policy()\n",
    "        self.evaluate_value()\n",
    "        #policy improve\n",
    "        self.greedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = Dp_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "dp.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1, error = 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.step_one_iter()\n",
    "#upgrade the policy once\n",
    "dp.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100.        ,   -1.        ,    1.625     ,   10.        ,\n",
       "          3.        ,   -1.125     ,   -0.9375    ,   -0.625     ,\n",
       "         -1.1171875 ,  -38.38964844,   -5.87683105])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = right\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = left\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "dp.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Mc_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        self.alpha = 0.5\n",
    "        self.count = np.zeros(11)\n",
    "        self.k = 0\n",
    "        self.gridmap.value[self.gridmap.state_num_reward] = self.gridmap.r_reward\n",
    "        self.gridmap.value[self.gridmap.state_num_penalty] = self.gridmap.r_penalty\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "\n",
    "    def sample_mc_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        initial_state = random.randint(0,10)\n",
    "        while initial_state == self.gridmap.state_num_reward or initial_state == self.gridmap.state_num_penalty:\n",
    "            initial_state = random.randint(0,10)\n",
    "            \n",
    "        state = initial_state\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        return_ = 0\n",
    "        while(state != self.gridmap.state_num_penalty and state != self.gridmap.state_num_reward):\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.gridmap.policy[:,state])[0]\n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            next_state = np.random.choice(11,1,p=self.gridmap.transition_mat[action][:,state])[0]\n",
    "            reward = self.gridmap.get_reward(state,next_state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        return states,actions,rewards\n",
    "        \n",
    "    def calculate_return_from_episode(self,rewards):\n",
    "        #calculate the return from the generated episode\n",
    "        return_ = 0\n",
    "        for i,reward in enumerate(rewards):\n",
    "            return_ += self.gridmap.gamma**i * reward\n",
    "        return return_\n",
    "            \n",
    "    def update_mc_episode(self,states,actions,rewards):\n",
    "        #update the value of the agent wrt to an episode\n",
    "        #update only for the first visit\n",
    "        visited = []\n",
    "        for state in states:\n",
    "            if not state in visited:\n",
    "                visited.append(state)                \n",
    "                self.count[state] += 1\n",
    "                return_ = self.calculate_return_from_episode(rewards)\n",
    "                self.gridmap.value[state] += (return_-self.gridmap.value[state])/(self.count[state]*1.0)\n",
    "        \n",
    "    def mc_prediction(self,n,epsilon=0.0):\n",
    "        #evaluate the value function of a policy by finding a sample and updating\n",
    "        for i in range(n):\n",
    "            if i % 1000 == 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_mc_episode(epsilon)\n",
    "            self.update_mc_episode(states,actions,rewards)\n",
    "        \n",
    "    def mc_control(self):\n",
    "        #improve the policy from the value function\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "            \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "        \n",
    "           \n",
    "    def mc_iterate(self,n,episode_per_step):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.k += 1\n",
    "            self.value = np.zeros(11)\n",
    "            print('===============================')\n",
    "            print('>',i,'th iter')\n",
    "            print('===============================')\n",
    "            self.mc_prediction(episode_per_step,epsilon=1.0/self.k)\n",
    "            self.mc_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = Mc_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "mc.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 0 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 1 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 2 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 3 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 4 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 5 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 6 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 7 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 8 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 9 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 10 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 11 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 12 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 13 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 14 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 15 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 16 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 17 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 18 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 19 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 20 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 21 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 22 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 23 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 24 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 25 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 26 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 27 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 28 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 29 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 30 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 31 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 32 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 33 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 34 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 35 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 36 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 37 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 38 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 39 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 40 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 41 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 42 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 43 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 44 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 45 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 46 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 47 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 48 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 49 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 50 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 51 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 52 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 53 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 54 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 55 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 56 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 57 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 58 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 59 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 60 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 61 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 62 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 63 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 64 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 65 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 66 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 67 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 68 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 69 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 70 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 71 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 72 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 73 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 74 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 75 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 76 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 77 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 78 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 79 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 80 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 81 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 82 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 83 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 84 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 85 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 86 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 87 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 88 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 89 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 90 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 91 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 92 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 93 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 94 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 95 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 96 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 97 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 98 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 99 th iter\n",
      "===============================\n",
      "performing 0th episode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc.mc_iterate(100,1)\n",
    "#upgrade the policy once\n",
    "mc.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100.        ,   -0.4215615 ,    1.60439236,   10.        ,\n",
       "          1.94082559,   -1.17214012,   -0.88849386,   -0.13959961,\n",
       "         -1.48266983,  -18.74400024,   -4.43655396])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = left\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "mc.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Td_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        self.alpha = 0.5\n",
    "        self.count = np.zeros(11)\n",
    "        self.k = 0\n",
    "        self.gridmap.value[self.gridmap.state_num_reward] = self.gridmap.r_reward\n",
    "        self.gridmap.value[self.gridmap.state_num_penalty] = self.gridmap.r_penalty\n",
    "        self.how_to_go = {12:3,\n",
    "                         15:1,\n",
    "                         23:3,\n",
    "                         21:2,\n",
    "                         26:1,\n",
    "                         43:2,\n",
    "                         47:1,\n",
    "                         51:0,\n",
    "                         56:3,\n",
    "                         62:0,\n",
    "                         65:2,\n",
    "                         68:1,\n",
    "                         74:0,\n",
    "                         710:1,\n",
    "                         86:0,\n",
    "                         89:3,\n",
    "                         98:2,\n",
    "                         910:3,\n",
    "                         90:1,\n",
    "                         109:2,\n",
    "                         107:0}\n",
    "\n",
    "    def sample_td_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        initial_state = random.randint(0,10)\n",
    "        while initial_state == self.gridmap.state_num_reward or initial_state == self.gridmap.state_num_penalty:\n",
    "            initial_state = random.randint(0,10)\n",
    "            \n",
    "        state = initial_state\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        return_ = 0\n",
    "        while(state != self.gridmap.state_num_penalty and state != self.gridmap.state_num_reward):\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.gridmap.policy[:,state])[0]\n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            next_state = np.random.choice(11,1,p=self.gridmap.transition_mat[action][:,state])[0]\n",
    "            reward = self.gridmap.get_reward(state,next_state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "        return states,actions,rewards\n",
    "        \n",
    "    def calculate_return_from_episode(self,rewards):\n",
    "        #calculate the return from the generated episode\n",
    "        return_ = 0\n",
    "        for i,reward in enumerate(rewards):\n",
    "            return_ += self.gridmap.gamma**i * reward\n",
    "        return return_\n",
    "            \n",
    "    def update_td_episode(self,states,actions,rewards):\n",
    "        #update the value of the agent wrt to an episode\n",
    "        #update only for the first visit\n",
    "        visited = []\n",
    "        for (i,state) in enumerate(states):\n",
    "            if i+1 == len(states):\n",
    "                break\n",
    "            if not state in visited:\n",
    "                visited.append(state)                \n",
    "                self.count[state] += 1\n",
    "                bootstrapped_return = rewards[i]+self.gridmap.gamma*self.gridmap.value[states[i+1]]\n",
    "                self.gridmap.value[state] += (bootstrapped_return-self.gridmap.value[state])/(self.count[state]*1.0)\n",
    "        \n",
    "    def td_prediction(self,n,epsilon=0.0):\n",
    "        #evaluate the value function of a policy by finding a sample and updating\n",
    "        for i in range(n):\n",
    "            if i % 1000 == 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_td_episode(epsilon)\n",
    "            self.update_td_episode(states,actions,rewards)\n",
    "        \n",
    "    def td_control(self):\n",
    "        #improve the policy from the value function\n",
    "        self.gridmap.policy = np.zeros((4,11))\n",
    "        for from_state in range(0,11):\n",
    "            if from_state == self.gridmap.state_num_reward or from_state == self.gridmap.state_num_penalty:\n",
    "                #print(\"state %s is not a normal state\"%(from_state))\n",
    "                self.gridmap.policy[:,from_state] = 0\n",
    "                continue\n",
    "            to_states = []\n",
    "            max_value = -9999\n",
    "            best_action = -1\n",
    "            for action in range(0,4):\n",
    "                possible_states = self.gridmap.get_possible_state_from(from_state,action)\n",
    "                for possible_state in possible_states:\n",
    "                    if (not (possible_state[0] in to_states)) and (possible_state[0] != from_state):\n",
    "                        to_states.append(possible_state[0])\n",
    "            for to_state in to_states:\n",
    "                #print(to_state)\n",
    "                to_state = int(to_state)\n",
    "                if self.gridmap.value[int(to_state)] > max_value:\n",
    "                    max_value = self.gridmap.value[to_state]\n",
    "                    best_action = self.how_to_go_to(from_state,to_state)\n",
    "            self.gridmap.policy[best_action,from_state] = 1\n",
    "            #print(\"best action for state %s is %s, and is set to %s\"%(from_state,best_action,self.gridmap.policy[best_action,from_state]))\n",
    "            #print(self.gridmap.policy)\n",
    "            \n",
    "    def how_to_go_to(self,from_state,to_state):\n",
    "        s = str(from_state)+str(to_state)\n",
    "        s = int(s)\n",
    "        if not s in self.how_to_go:\n",
    "            print(\"moving from %s to %s is impossible\"%(from_state,to_state))\n",
    "            return -1\n",
    "        else:\n",
    "            return self.how_to_go[s]\n",
    "        \n",
    "           \n",
    "    def td_iterate(self,n,episode_per_step):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.k += 1\n",
    "            self.value = np.zeros(11)\n",
    "            print('===============================')\n",
    "            print('>',i,'th iter')\n",
    "            print('===============================')\n",
    "            self.td_prediction(episode_per_step,epsilon=1.0/self.k)\n",
    "            self.td_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = Td_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "td.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 0 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 1 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 2 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 3 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 4 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 5 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 6 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 7 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 8 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 9 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 10 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 11 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 12 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 13 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 14 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 15 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 16 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 17 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 18 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 19 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 20 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 21 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 22 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 23 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 24 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 25 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 26 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 27 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 28 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 29 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 30 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 31 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 32 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 33 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 34 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 35 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 36 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 37 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 38 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 39 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 40 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 41 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 42 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 43 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 44 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 45 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 46 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 47 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 48 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 49 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 50 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 51 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 52 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 53 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 54 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 55 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 56 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 57 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 58 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 59 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 60 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 61 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 62 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 63 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 64 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 65 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 66 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 67 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 68 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 69 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 70 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 71 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 72 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 73 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 74 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 75 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 76 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 77 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 78 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 79 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 80 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 81 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 82 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 83 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 84 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 85 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 86 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 87 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 88 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 89 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 90 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 91 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 92 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 93 th iter\n",
      "===============================\n",
      "performing 0th episode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 94 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 95 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 96 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 97 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 98 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "===============================\n",
      "> 99 th iter\n",
      "===============================\n",
      "performing 0th episode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.td_iterate(100,1)\n",
    "#upgrade the policy once\n",
    "td.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-100.        ,   -1.83372483,   -1.82282106,   10.        ,\n",
       "         -1.84612884,   -1.82835008,   -1.83667002,   -1.83025326,\n",
       "         -1.84002516,   -1.83675884,   -1.83711284])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td.gridmap.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = down\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = right\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "td.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Sarsa_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        self.alpha = 0.5\n",
    "        self.count = np.zeros((4,11))\n",
    "        self.count[:,self.gridmap.state_num_reward]=1\n",
    "        self.count[:,self.gridmap.state_num_penalty]=1\n",
    "        self.t = 0\n",
    "        self.gridmap.action_value[:,self.gridmap.state_num_reward] = 0\n",
    "        self.gridmap.action_value[:,self.gridmap.state_num_penalty] = 0\n",
    "\n",
    "    def sample_sarsa_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        initial_state = random.randint(0,10)\n",
    "        while initial_state == self.gridmap.state_num_reward or initial_state == self.gridmap.state_num_penalty:\n",
    "            initial_state = random.randint(0,10)\n",
    "            \n",
    "        state = initial_state\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        return_ = 0\n",
    "        max_i = 20\n",
    "        i=0\n",
    "        next_action = -1\n",
    "        while(state != self.gridmap.state_num_penalty and state != self.gridmap.state_num_reward and i<max_i):\n",
    "            i+=1\n",
    "            #choosing action\n",
    "            if next_action == -1:\n",
    "                explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "                action = np.random.choice(4,1,p=self.gridmap.policy[:,state])[0]\n",
    "                if explore == 1:\n",
    "                    action = np.random.choice(4,1)[0]\n",
    "            else:\n",
    "                action = next_action\n",
    "            #moving to the next state\n",
    "            next_state = np.random.choice(11,1,p=self.gridmap.transition_mat[action][:,state])[0]\n",
    "            reward = self.gridmap.get_reward(state,next_state)\n",
    "            #choosing next action\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            next_action = np.random.choice(4,1,p=self.gridmap.policy[:,state])[0]\n",
    "            if explore == 1:\n",
    "                next_action = np.random.choice(4,1)[0] \n",
    "            #update\n",
    "            self.update_sarsa_episode(state,action,reward,next_state,next_action)\n",
    "            state = next_state\n",
    "        return states,actions,rewards\n",
    "            \n",
    "    def update_sarsa_episode(self,state,action,reward,next_state,next_action):\n",
    "        self.count[action,state] += 1\n",
    "        alpha = 1.0/self.t\n",
    "        #alpha = 0.5             \n",
    "        next_q = self.gridmap.action_value[next_action,next_state]\n",
    "        bootstrapped_return = reward+self.gridmap.gamma*next_q\n",
    "        #print(\"--------------------------------\")\n",
    "        #print(\"alpha = %s: from %s, do action %s (action value = %s)\"%(alpha,state,action,self.gridmap.action_value[action,state]))\n",
    "        #print(next_state)\n",
    "        #print(\"do action %s, reward = %s (next action value = %s)\"%(next_action,rewards[i],next_q))\n",
    "        #print(\"--------------------------------\")\n",
    "        self.gridmap.action_value[action,state] += alpha*(bootstrapped_return-self.gridmap.action_value[action,state])\n",
    "\n",
    "    def visited_all_states(self):\n",
    "        #print(self.count)\n",
    "        return not np.any(self.count==0)\n",
    "    \n",
    "    def sarsa_prediction(self,epsilon=0.0):\n",
    "        #evaluate the action value function of a policy by finding a sample and updating\n",
    "        i=0\n",
    "        while not self.visited_all_states():\n",
    "            if i % 1000 == 0 and i != 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_sarsa_episode(epsilon)\n",
    "            i+=1\n",
    "        self.count = np.zeros((4,11))\n",
    "        self.count[:,self.gridmap.state_num_reward]=1\n",
    "        self.count[:,self.gridmap.state_num_penalty]=1\n",
    "            \n",
    "    def sarsa_control(self):\n",
    "        #improve the policy from the action value function\n",
    "        self.gridmap.policy = np.zeros_like(self.gridmap.policy)\n",
    "        for state in range(11):\n",
    "            q_up = self.gridmap.action_value[0,state]\n",
    "            q_down = self.gridmap.action_value[1,state]\n",
    "            q_left = self.gridmap.action_value[2,state]\n",
    "            q_right = self.gridmap.action_value[3,state]\n",
    "            q_max = np.argmax(np.array([q_up,q_down,q_left,q_right]),0)\n",
    "            self.gridmap.policy[q_max,state] = 1\n",
    "           \n",
    "    def sarsa_iterate(self,n):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.t += 1\n",
    "            self.value = np.zeros(11)\n",
    "            if i%1 == 0:\n",
    "                print('===============================')\n",
    "                print('>',i,'th iter')\n",
    "                print('===============================')\n",
    "            self.sarsa_prediction(epsilon=1.0/self.t)\n",
    "            self.sarsa_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = Sarsa_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa.gridmap.action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "sarsa.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 0 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 1 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 2 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 3 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 4 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 5 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 6 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 7 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 8 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 9 th iter\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa.sarsa_iterate(10)\n",
    "#upgrade the policy once\n",
    "sarsa.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   -0.9   2.7   0.    3.4  -1.5  -0.4   0.1  -4.4 -19.6  -2.5]\n",
      " [  0.   -0.7   1.7   0.    0.3  -1.6  -2.4  -2.3  -4.  -73.9  -7.9]\n",
      " [  0.   -0.5   1.6   0.    7.9  -1.6  -1.8  -1.   -5.2 -14.1 -10.6]\n",
      " [  0.    0.    8.2   0.    4.2  -1.4  -1.5  -1.1  -7.5 -16.1  -5.8]]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(sarsa.gridmap.action_value,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = right\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = down\n",
      "on state = 9 , do action = left\n",
      "on state = 10 , do action = up\n"
     ]
    }
   ],
   "source": [
    "sarsa.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Q_solver:\n",
    "    def __init__(self,gridmap):\n",
    "        self.gridmap = gridmap\n",
    "        self.alpha = 0.5\n",
    "        self.count = np.zeros((4,11))\n",
    "        self.count[:,self.gridmap.state_num_reward]=1\n",
    "        self.count[:,self.gridmap.state_num_penalty]=1\n",
    "        self.t = 0\n",
    "        self.gridmap.action_value[:,self.gridmap.state_num_reward] = 0\n",
    "        self.gridmap.action_value[:,self.gridmap.state_num_penalty] = 0\n",
    "\n",
    "    def sample_Q_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        initial_state = random.randint(0,10)\n",
    "        while initial_state == self.gridmap.state_num_reward or initial_state == self.gridmap.state_num_penalty:\n",
    "            initial_state = random.randint(0,10)\n",
    "            \n",
    "        state = initial_state\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        return_ = 0\n",
    "        max_i = 20\n",
    "        i=0\n",
    "        while(state != self.gridmap.state_num_penalty and state != self.gridmap.state_num_reward and i<max_i):\n",
    "            i+=1\n",
    "            #choosing action\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.gridmap.policy[:,state])[0]\n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            #moving to the next state\n",
    "            next_state = np.random.choice(11,1,p=self.gridmap.transition_mat[action][:,state])[0]\n",
    "            reward = self.gridmap.get_reward(state,next_state)\n",
    "            #update\n",
    "            self.update_Q_episode(state,action,reward,next_state)\n",
    "            state = next_state\n",
    "        return states,actions,rewards\n",
    "            \n",
    "    def update_Q_episode(self,state,action,reward,next_state):\n",
    "        self.count[action,state] += 1\n",
    "        alpha = 1.0/self.t\n",
    "        #alpha = 0.5             \n",
    "        next_q = self.get_best_q_from(next_state)\n",
    "        bootstrapped_return = reward+self.gridmap.gamma*next_q\n",
    "        #print(\"--------------------------------\")\n",
    "        #print(\"alpha = %s: from %s, do action %s (action value = %s)\"%(alpha,state,action,self.gridmap.action_value[action,state]))\n",
    "        #print(next_state)\n",
    "        #print(\"do action %s, reward = %s (next action value = %s)\"%(next_action,rewards[i],next_q))\n",
    "        #print(\"--------------------------------\")\n",
    "        self.gridmap.action_value[action,state] += alpha*(bootstrapped_return-self.gridmap.action_value[action,state])\n",
    "\n",
    "    def get_best_q_from(self,state):\n",
    "        q_up = self.gridmap.action_value[0,state]\n",
    "        q_down = self.gridmap.action_value[1,state]\n",
    "        q_left = self.gridmap.action_value[2,state]\n",
    "        q_right = self.gridmap.action_value[3,state]\n",
    "        return np.max(np.array([q_up,q_down,q_left,q_right]),0)\n",
    "        \n",
    "    def visited_all_states(self):\n",
    "        #print(self.count)\n",
    "        return not np.any(self.count==0)\n",
    "    \n",
    "    def Q_prediction(self,epsilon=0.0):\n",
    "        #evaluate the action value function of a policy by finding a sample and updating\n",
    "        i=0\n",
    "        while not self.visited_all_states():\n",
    "            if i % 1000 == 0 and i != 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_Q_episode(epsilon)\n",
    "            i+=1\n",
    "        self.count = np.zeros((4,11))\n",
    "        self.count[:,self.gridmap.state_num_reward]=1\n",
    "        self.count[:,self.gridmap.state_num_penalty]=1\n",
    "            \n",
    "    def Q_control(self):\n",
    "        #improve the policy from the action value function\n",
    "        self.gridmap.policy = np.zeros_like(self.gridmap.policy)\n",
    "        for state in range(11):\n",
    "            q_up = self.gridmap.action_value[0,state]\n",
    "            q_down = self.gridmap.action_value[1,state]\n",
    "            q_left = self.gridmap.action_value[2,state]\n",
    "            q_right = self.gridmap.action_value[3,state]\n",
    "            q_max = np.argmax(np.array([q_up,q_down,q_left,q_right]),0)\n",
    "            self.gridmap.policy[q_max,state] = 1\n",
    "           \n",
    "    def Q_iterate(self,n):\n",
    "        #learn the gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.t += 1\n",
    "            self.value = np.zeros(11)\n",
    "            if i%1 == 0:\n",
    "                print('===============================')\n",
    "                print('>',i,'th iter')\n",
    "                print('===============================')\n",
    "            self.Q_prediction(epsilon=1.0/self.t)\n",
    "            self.Q_control()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q_solver(Gridmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.gridmap.action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
       "       [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see initial policy\n",
    "q.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 0 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 1 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 2 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 3 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 4 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 5 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 6 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 7 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 8 th iter\n",
      "===============================\n",
      "===============================\n",
      "> 9 th iter\n",
      "===============================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.Q_iterate(10)\n",
    "#upgrade the policy once\n",
    "q.gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   -0.4   3.5   0.    4.2  -0.5   1.1   0.9  -1.5 -21.3  -1.7]\n",
      " [  0.   -0.5   1.8   0.    1.1  -1.   -0.2  -0.7  -3.  -52.4  -1.5]\n",
      " [  0.   -0.5   1.2   0.    6.5  -1.1  -0.6  -0.   -2.5 -11.1  -3.6]\n",
      " [  0.    1.3   6.3   0.    3.2  -0.6  -0.4  -0.1  -4.6 -12.9  -1.1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(q.gridmap.action_value,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = left\n",
      "on state = 10 , do action = right\n"
     ]
    }
   ],
   "source": [
    "q.gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
