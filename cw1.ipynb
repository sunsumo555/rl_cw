{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_length(cid):\n",
    "    cid_str = str(cid)\n",
    "    return len(cid_str)\n",
    "\n",
    "def get_digit(cid,digit):\n",
    "    cid_str = str(cid)\n",
    "    return int(cid_str[digit-1])\n",
    "\n",
    "def s_t(cid,t):\n",
    "    return (get_digit(cid,t)+1)%3\n",
    "\n",
    "def r_t_plus1(cid,t):\n",
    "    return (get_digit(cid,t))%2\n",
    "\n",
    "def find_sequence(cid):\n",
    "    length = find_length(cid)\n",
    "    sequence = ''\n",
    "    for i in range(1,length+1):\n",
    "        sequence = sequence + 's_' + str(s_t(cid,i)) + ' '\n",
    "        sequence = sequence + str(r_t_plus1(cid,i)) + ' '\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s_2 1 s_1 0 s_1 0 s_0 1 s_0 0 s_1 0 s_2 0 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sequence(1605864)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.) 1. what can we infer about the structure of the transition matrix?\n",
    "\n",
    "the transition matrix consists of 3 states, which will result in a 3x3 matrix as the only action available is no action.\n",
    "\n",
    "from the data given, one possible configuration of a transition matrix is\n",
    "\n",
    "there is 1 time that we move from s1 to s1\n",
    "there is 1 time that we move from s1 to s2\n",
    "there is 1 time that we move from s2 to s0\n",
    "\n",
    "       from\n",
    "  s0   s1   s2\n",
    "[0.0   0.0  1.0 ] s0\n",
    "[0.0   0.5  0.0 ] s1 to\n",
    "[0.0   0.5  0.0 ] s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.) 2. what can we infer about structure of the reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.) The reward function at s0 is 0, this is because s0 cannot move to any other states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_num_reward = 3\n",
      "state_num_penalty = 0\n",
      "p = 0.65\n",
      "gamma = 0.5\n"
     ]
    }
   ],
   "source": [
    "x=8\n",
    "y=6\n",
    "z=4\n",
    "state_num_reward = ((z+1)%3)+1\n",
    "state_num_penalty = 0\n",
    "r_reward = 10\n",
    "r_penalty = -100\n",
    "r_transition = -1\n",
    "p = 0.25+0.5*x/10.0\n",
    "gamma = 0.2+0.5*y/10.0\n",
    "\n",
    "print('state_num_reward =',state_num_reward)\n",
    "print('state_num_penalty =',state_num_penalty)\n",
    "print('p =',p)\n",
    "print('gamma =',gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, s11 is renamed to be s0, all other states numbers are the same as the assignment\n",
    "\n",
    "class Gridmap_mdp:\n",
    "    def __init__(self,grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma):\n",
    "        #initialize all of the attributes\n",
    "        self.state_num_reward = state_num_reward\n",
    "        self.state_num_penalty = state_num_penalty\n",
    "        self.r_reward = r_reward\n",
    "        self.r_penalty = r_penalty\n",
    "        self.r_transition = r_transition\n",
    "        self.p = p\n",
    "        self.gamma = gamma\n",
    "        self.grid_map = grid_map\n",
    "        \n",
    "        self.count = np.zeros([4,11])\n",
    "        self.policy = np.ones([4,11])*0.25\n",
    "        self.action_value = np.zeros([4,11])\n",
    "        self.value = np.zeros([11])\n",
    "        self.locations_dict = self.create_location_dict(self.grid_map)\n",
    "        self.transition_mat = self.init_transition_mat()\n",
    "        \n",
    "    #action: 0=up 1=down 2=left 3=right\n",
    "    def init_transition_mat(self):\n",
    "        #create the transition matrix\n",
    "        transition_mat = []\n",
    "        for action in ['u','d','l','r']:\n",
    "            transition_mat.append(self.init_transition_mat_direction(action))\n",
    "            \n",
    "        return transition_mat\n",
    "        \n",
    "    def create_location_dict(self,grid_map):\n",
    "        #create a dictionary to map state to coordinates\n",
    "        locations_dict = {}\n",
    "        for i in range(grid_map.shape[0]):\n",
    "            for j in range(grid_map.shape[1]):\n",
    "                # store as x and y\n",
    "                locations_dict[grid_map[i][j]] = [j,i]\n",
    "        return locations_dict\n",
    "        \n",
    "    def increment_where(self,from_,direction):\n",
    "        #find where should the probability be increamented, used to help create transition matrix\n",
    "        [x,y] = self.locations_dict[from_]\n",
    "\n",
    "        if direction == 'r':\n",
    "            #indended direction: right\n",
    "            if x+1 >= 4 or self.grid_map[y,x+1] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y,x+1]]\n",
    "\n",
    "        if direction == 'l':\n",
    "            #indended direction: left\n",
    "            if x-1 < 0 or self.grid_map[y,x-1] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y,x-1]]\n",
    "\n",
    "        if direction == 'u':\n",
    "            #indended direction: up\n",
    "            if y-1 < 0 or self.grid_map[y-1,x] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y-1,x]]\n",
    "\n",
    "        if direction == 'd':\n",
    "            #indended direction: down\n",
    "            if y+1 >= 4 or self.grid_map[y+1,x] == -1:\n",
    "                return [from_,from_]\n",
    "            else:\n",
    "                return [from_,self.grid_map[y+1,x]]\n",
    "\n",
    "    def init_transition_mat_direction(self,desired_action):\n",
    "        #create a transition matrix for only for the desired_action\n",
    "        action_list = ['u','d','l','r']\n",
    "        transition_mat = np.zeros([11,11])\n",
    "        for from_ in range(11):\n",
    "            for action in action_list:\n",
    "                update_from, update_to = self.increment_where(from_,action)\n",
    "                if action == desired_action:\n",
    "                    transition_mat[update_to,update_from] += self.p\n",
    "                else:\n",
    "                    transition_mat[update_to,update_from] += (1-self.p)/3.0\n",
    "        return transition_mat\n",
    "    \n",
    "    def get_reward(self,from_,to_):\n",
    "        #calculate the reward gained from moving from a state to another\n",
    "        if to_ == state_num_penalty:\n",
    "            return self.r_penalty\n",
    "        elif to_ == state_num_reward:\n",
    "            return self.r_reward\n",
    "        else:\n",
    "            return self.r_transition\n",
    "        \n",
    "    def get_possible_state_from(self,state_from,action):\n",
    "        #find what states are possible from a state and action\n",
    "        possible_states = []\n",
    "        #0=up 1=down 2=left 3=right\n",
    "        transition_mat = self.transition_mat[action]\n",
    "        for i in range(transition_mat.shape[1]):\n",
    "            if transition_mat[i,state_from] > 0.0:\n",
    "                state_to = i\n",
    "                p_to_that_state = transition_mat[i,state_from]\n",
    "                possible_states.append(np.array([state_to,p_to_that_state]))\n",
    "\n",
    "    def sample_mc_episode(self,epsilon=0.0):\n",
    "        #draw a sample episode from the environment\n",
    "        self.alpha = 0.5\n",
    "        initial_state = np.random.choice(9, 1)[0]+2\n",
    "        state = initial_state\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        \n",
    "        return_ = 0\n",
    "        \n",
    "        while(state != state_num_penalty and state != self.state_num_reward):\n",
    "            explore = np.random.choice(2,1,p=[1-epsilon,epsilon])[0]\n",
    "            action = np.random.choice(4,1,p=self.policy[:,state])[0]\n",
    "            \n",
    "            if explore == 1:\n",
    "                action = np.random.choice(4,1)[0]\n",
    "            \n",
    "            next_state = np.random.choice(11,1,p=self.transition_mat[action][:,state])[0]\n",
    "            reward = self.get_reward(state,next_state)\n",
    "                            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            return_ += reward\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        return states,actions,rewards\n",
    "        \n",
    "    def calculate_return_from_episode(self,rewards):\n",
    "        #calculate the return from the generated episode\n",
    "        return_ = 0\n",
    "        for i,reward in enumerate(rewards):\n",
    "            return_ += self.gamma**i * reward\n",
    "        return return_\n",
    "            \n",
    "    def update_mc_episode(self,states,actions,rewards):\n",
    "        #update the action value of the agent wrt to an episode\n",
    "        for state,action in zip(states,actions):\n",
    "            self.count[action,state] += 1\n",
    "            return_ = self.calculate_return_from_episode(rewards)\n",
    "            self.action_value[action,state] = ((self.count[action,state]-1)*self.action_value[action,state] + return_)/(self.count[action,state]*1.0)\n",
    "        \n",
    "    def mc_policy_eval(self,n,epsilon=0.0):\n",
    "        #evaluate the action value function of a policy\n",
    "        for i in range(n):\n",
    "            if i % 1000 == 0:\n",
    "                print('performing %sth episode'%(i))\n",
    "            states,actions,rewards = self.sample_mc_episode(epsilon)\n",
    "            self.update_mc_episode(states,actions,rewards)\n",
    "        \n",
    "    def policy_improve(self):\n",
    "        #improve the policy from the action value function\n",
    "        self.policy = np.zeros_like(self.policy)\n",
    "        for state in range(11):\n",
    "            v_up = Gridmap.action_value[0,state]\n",
    "            v_down = Gridmap.action_value[1,state]\n",
    "            v_left = Gridmap.action_value[2,state]\n",
    "            v_right = Gridmap.action_value[3,state]\n",
    "\n",
    "            v_max = np.argmax(np.array([v_up,v_down,v_left,v_right]),0)\n",
    "\n",
    "            self.policy[v_max,state] = 1\n",
    "            \n",
    "    def tell_policy(self):\n",
    "        #print the policy\n",
    "        action_word = {0:'up',1:'down',2:'left',3:'right'}\n",
    "        for state in range(11):\n",
    "            v_up = Gridmap.action_value[0,state]\n",
    "            v_down = Gridmap.action_value[1,state]\n",
    "            v_left = Gridmap.action_value[2,state]\n",
    "            v_right = Gridmap.action_value[3,state]\n",
    "\n",
    "            v_max = np.argmax(np.array([v_up,v_down,v_left,v_right]),0)\n",
    "            \n",
    "            print('on state =',state,', do action =',action_word[v_max])\n",
    "            \n",
    "    def mc_learn(self,n,episode_per_step,epsilon=0.0):\n",
    "        #learn he gridmap with epsilon greedy montecarlo\n",
    "        for i in range(n):\n",
    "            self.action_value = np.zeros([4,11])\n",
    "            print('===============================')\n",
    "            print('>',i,'th iter')\n",
    "            print('===============================')\n",
    "            self.mc_policy_eval(episode_per_step,epsilon)\n",
    "            self.policy_improve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_map = np.array([[1,2,3,4],\n",
    "                    [5,6,-1,7],\n",
    "                    [-1,8,9,10],\n",
    "                    [-1,-1,0,-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gridmap = Gridmap_mdp(grid_map,state_num_reward,state_num_penalty,r_reward,r_penalty,r_transition,p,gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = 5 ; action = 2 ; rewarded = -1\n",
      "state = 5 ; action = 3 ; rewarded = -1\n",
      "state = 6 ; action = 0 ; rewarded = -1\n",
      "state = 6 ; action = 1 ; rewarded = -1\n",
      "state = 6 ; action = 1 ; rewarded = -1\n",
      "state = 8 ; action = 3 ; rewarded = -1\n",
      "state = 9 ; action = 1 ; rewarded = -100\n"
     ]
    }
   ],
   "source": [
    "states,actions,rewards = Gridmap.sample_mc_episode(0.3)\n",
    "for state,action,reward in zip(states,actions,rewards):\n",
    "    print('state =',state,'; action =',action,'; rewarded =',reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gridmap.action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = 0 action = 0\n",
      "state = 1 action = 0\n",
      "state = 2 action = 0\n",
      "state = 3 action = 0\n",
      "state = 4 action = 0\n",
      "state = 5 action = 0\n",
      "state = 6 action = 0\n",
      "state = 7 action = 0\n",
      "state = 8 action = 0\n",
      "state = 9 action = 0\n",
      "state = 10 action = 0\n"
     ]
    }
   ],
   "source": [
    "for state in range(11):\n",
    "    v_up = Gridmap.action_value[0,state]\n",
    "    v_down = Gridmap.action_value[1,state]\n",
    "    v_left = Gridmap.action_value[2,state]\n",
    "    v_right = Gridmap.action_value[3,state]\n",
    "    \n",
    "    v_max = np.argmax(np.array([v_up,v_down,v_left,v_right]),0)\n",
    "    \n",
    "    print('state =',state,'action =',v_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "> 0 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "performing 1000th episode\n",
      "performing 2000th episode\n",
      "performing 3000th episode\n",
      "performing 4000th episode\n",
      "performing 5000th episode\n",
      "performing 6000th episode\n",
      "performing 7000th episode\n",
      "performing 8000th episode\n",
      "performing 9000th episode\n",
      "performing 10000th episode\n",
      "performing 11000th episode\n",
      "performing 12000th episode\n",
      "performing 13000th episode\n",
      "performing 14000th episode\n",
      "performing 15000th episode\n",
      "performing 16000th episode\n",
      "performing 17000th episode\n",
      "performing 18000th episode\n",
      "performing 19000th episode\n",
      "===============================\n",
      "> 1 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "performing 1000th episode\n",
      "performing 2000th episode\n",
      "performing 3000th episode\n",
      "performing 4000th episode\n",
      "performing 5000th episode\n",
      "performing 6000th episode\n",
      "performing 7000th episode\n",
      "performing 8000th episode\n",
      "performing 9000th episode\n",
      "performing 10000th episode\n",
      "performing 11000th episode\n",
      "performing 12000th episode\n",
      "performing 13000th episode\n",
      "performing 14000th episode\n",
      "performing 15000th episode\n",
      "performing 16000th episode\n",
      "performing 17000th episode\n",
      "performing 18000th episode\n",
      "performing 19000th episode\n",
      "===============================\n",
      "> 2 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "performing 1000th episode\n",
      "performing 2000th episode\n",
      "performing 3000th episode\n",
      "performing 4000th episode\n",
      "performing 5000th episode\n",
      "performing 6000th episode\n",
      "performing 7000th episode\n",
      "performing 8000th episode\n",
      "performing 9000th episode\n",
      "performing 10000th episode\n",
      "performing 11000th episode\n",
      "performing 12000th episode\n",
      "performing 13000th episode\n",
      "performing 14000th episode\n",
      "performing 15000th episode\n",
      "performing 16000th episode\n",
      "performing 17000th episode\n",
      "performing 18000th episode\n",
      "performing 19000th episode\n",
      "===============================\n",
      "> 3 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "performing 1000th episode\n",
      "performing 2000th episode\n",
      "performing 3000th episode\n",
      "performing 4000th episode\n",
      "performing 5000th episode\n",
      "performing 6000th episode\n",
      "performing 7000th episode\n",
      "performing 8000th episode\n",
      "performing 9000th episode\n",
      "performing 10000th episode\n",
      "performing 11000th episode\n",
      "performing 12000th episode\n",
      "performing 13000th episode\n",
      "performing 14000th episode\n",
      "performing 15000th episode\n",
      "performing 16000th episode\n",
      "performing 17000th episode\n",
      "performing 18000th episode\n",
      "performing 19000th episode\n",
      "===============================\n",
      "> 4 th iter\n",
      "===============================\n",
      "performing 0th episode\n",
      "performing 1000th episode\n",
      "performing 2000th episode\n",
      "performing 3000th episode\n",
      "performing 4000th episode\n",
      "performing 5000th episode\n",
      "performing 6000th episode\n",
      "performing 7000th episode\n",
      "performing 8000th episode\n",
      "performing 9000th episode\n",
      "performing 10000th episode\n",
      "performing 11000th episode\n",
      "performing 12000th episode\n",
      "performing 13000th episode\n",
      "performing 14000th episode\n",
      "performing 15000th episode\n",
      "performing 16000th episode\n",
      "performing 17000th episode\n",
      "performing 18000th episode\n",
      "performing 19000th episode\n"
     ]
    }
   ],
   "source": [
    "Gridmap.mc_learn(5,20000,epsilon=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 0 , do action = up\n",
      "on state = 1 , do action = left\n",
      "on state = 2 , do action = right\n",
      "on state = 3 , do action = up\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = right\n",
      "on state = 6 , do action = right\n",
      "on state = 7 , do action = right\n",
      "on state = 8 , do action = left\n",
      "on state = 9 , do action = right\n",
      "on state = 10 , do action = right\n"
     ]
    }
   ],
   "source": [
    "Gridmap.tell_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on state = 0 , do action = up\n",
      "on state = 1 , do action = right\n",
      "on state = 2 , do action = right\n",
      "on state = 3 , do action = up\n",
      "on state = 4 , do action = left\n",
      "on state = 5 , do action = up\n",
      "on state = 6 , do action = up\n",
      "on state = 7 , do action = up\n",
      "on state = 8 , do action = up\n",
      "on state = 9 , do action = left\n",
      "on state = 10 , do action = up\n",
      "None\n",
      "==================\n",
      "at s6 go up landed at s2 and got -1\n",
      "at s2 go right landed at s6 and got -1\n",
      "at s6 go up landed at s8 and got -1\n",
      "at s8 go up landed at s6 and got -1\n",
      "at s6 go up landed at s2 and got -1\n",
      "got 10 reward for ending after s2\n"
     ]
    }
   ],
   "source": [
    "states, actions, rewards = Gridmap.sample_mc_episode()\n",
    "action_word = {0:'up',1:'down',2:'left',3:'right'}\n",
    "\n",
    "print(Gridmap.tell_policy())\n",
    "print('==================')\n",
    "for i in range(len(states)-1):\n",
    "    if i == len(states):\n",
    "        print('at s%s go %s landed at last state'%(states[i],action_word[actions[i]]))\n",
    "        break\n",
    "        \n",
    "    print('at s%s go %s landed at s%s and got %s'%(states[i],action_word[actions[i]],states[i+1],rewards[i]))\n",
    "    \n",
    "print('got %s reward for ending after s%s'%(rewards[len(rewards)-1],states[len(states)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gridmap.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value of s0: up: 0.0, down: 0.0, left: 0.0, right: 0.0\n",
      "value of s1: up: -0.1709568587381877, down: -0.16867573416580217, left: -0.6635417520329808, right: -0.06900226030585666\n",
      "value of s2: up: -0.09250346739810446, down: -0.12412168030957911, left: -0.12077595496855949, right: -0.008867438825192898\n",
      "value of s3: up: 0.0, down: 0.0, left: 0.0, right: 0.0\n",
      "value of s4: up: -0.1342282867806504, down: -0.18103350090972775, left: -0.04182097062153927, right: -0.1290167929882382\n",
      "value of s5: up: -0.08234129769299624, down: -0.16579733022479354, left: -0.16565707216007428, right: -0.6573631472419137\n",
      "value of s6: up: -0.13034785988961023, down: -0.33826545880287817, left: -0.3256543967116718, right: -0.8120276744779945\n",
      "value of s7: up: -0.19986550249376767, down: -0.4421853320505261, left: -0.4311619550784326, right: -0.8671645464545591\n",
      "value of s8: up: -0.18481117626509067, down: -0.3803007844405331, left: -0.9349681592769523, right: -0.47666848216450114\n",
      "value of s9: up: -0.6999915881075139, down: -1.781077495518218, left: -0.33555372638031805, right: -1.6094426749864787\n",
      "value of s10: up: -0.36653161959016844, down: -0.5657479372115366, left: -0.6534939522242257, right: -0.9814070396126603\n"
     ]
    }
   ],
   "source": [
    "for state in range(Gridmap.action_value.shape[1]):\n",
    "    print('value of s%s: up: %s, down: %s, left: %s, right: %s'%(state,Gridmap.action_value[0,state],Gridmap.action_value[1,state],Gridmap.action_value[2,state],Gridmap.action_value[3,state]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
